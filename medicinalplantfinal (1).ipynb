{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"R6WLfHTYF7Vk"},"outputs":[],"source":["!pip install kaggle\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ipJIJYiSt3-1"},"outputs":[],"source":["pip install gradio\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mF_qyCX_t99v"},"outputs":[],"source":["import gradio as gr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pqGRimvSGHI0"},"outputs":[],"source":["from google.colab import files\n","files.upload()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubpqwpe-GWNj"},"outputs":[],"source":["!mkdir ~/.kaggle\n","!mv kaggle.json ~/.kaggle/\n","#casting envi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x81cZjfYHB6i"},"outputs":[],"source":["!kaggle datasets download -d aryashah2k/indian-medicinal-leaves-dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W11goAA_IjG1"},"outputs":[],"source":["!unzip /content/indian-medicinal-leaves-dataset.zip\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k27jQaR7HSn0"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow_hub as hub\n","import tensorflow_datasets as tfds\n","from tensorflow.keras import layers\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow_hub as hub\n","import tensorflow_datasets as tfds\n","from tensorflow.keras import layers\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import tensorflow as tf\n","from tensorflow import keras\n","import cv2\n","import os\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z2ByRrQhHhGl"},"outputs":[],"source":["dataset_path= '/content/Indian Medicinal Leaves Image Datasets/Medicinal Leaf dataset'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HFsmf1EsHj5Z"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","from tensorflow.keras.models import Model\n","def load_and_preprocess_images(dataset_path, image_size=(128, 128)):\n","    images = []\n","    labels = []\n","#0=0 white 1=255 black 0.5=128 grey\n","    # Iterate through class directories\n","    for class_dir in os.listdir(dataset_path):\n","        class_path = os.path.join(dataset_path, class_dir)\n","        class_label = int(class_dir)  # Convert directory name to label if needed\n","\n","        # Iterate through images in the class directory\n","        for image_file in os.listdir(class_path):\n","            image_path = os.path.join(class_path, image_file)\n","\n","            # Read and preprocess the image\n","            image = cv2.imread(image_path)\n","            image = cv2.resize(image, image_size)\n","            image = image.astype(np.float32) / 255.0  # Normalize pixel values\n","\n","            # Append the image and label to lists\n","            images.append(image)\n","            labels.append(class_label)\n","\n","    return np.array(images), np.array(labels)\n","\n","\n","# Constants\n","IMAGE_RES = 224\n","BATCH_SIZE = 32\n","# Load the dataset\n","data = tf.keras.utils.image_dataset_from_directory(\n","    '/content/Indian Medicinal Leaves Image Datasets/Medicinal Leaf dataset',\n","    image_size=(IMAGE_RES, IMAGE_RES),\n","    batch_size=BATCH_SIZE,\n","    validation_split=0.2,  # 20% of the data will be used for testing\n","    subset=\"training\",     # Specify \"training\" to get the training subset\n","    seed=42,\n","    label_mode='int'\n",")\n","num_examples = data.cardinality().numpy()\n","num_classes = len(data.class_names)\n","\n","# Split the data into training and testing\n","testing_data = tf.keras.utils.image_dataset_from_directory(\n","    '/content/Indian Medicinal Leaves Image Datasets/Medicinal Leaf dataset',\n","    image_size=(IMAGE_RES, IMAGE_RES),\n","    batch_size=BATCH_SIZE,\n","    validation_split=0.2,  # No validation split for testing\n","    seed=42,\n","    label_mode='int',\n","    subset=\"validation\"    # Specify \"validation\" to get the testing subset\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"366KkRQPZR3i"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"tVWrTVBHV5Xr"},"source":["# New Section"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NM8N7FXiHtwA","collapsed":true},"outputs":[],"source":["IMAGE_RES = 224  # InceptionV3 input shape\n","BATCH_SIZE = 32\n","def format_image(image, label):\n","    image = tf.image.resize(image, (IMAGE_RES, IMAGE_RES)) / 255.0\n","    return image, label\n","train_data = data\n","# Define the validation split ratio\n","validation_split = 0.2\n","num_validation_samples = int(num_examples * validation_split)\n","validation_data = data.take(num_validation_samples)\n","\n","train_batches = train_data.map(format_image).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n","validation_batches = validation_data.map(format_image).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n","\n","# Create the base model (MobileNetV2)\n","base_model = tf.keras.applications.MobileNetV2(\n","    input_shape=(IMAGE_RES, IMAGE_RES, 3),\n","    include_top=False,\n","    weights='imagenet'\n",")\n","\n","# Freeze the base model\n","base_model.trainable = False\n","\n","# Add custom layers for classification\n","global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n","prediction_layer = tf.keras.layers.Dense(num_classes, activation='softmax')\n","\n","# Create the final model\n","model = tf.keras.Sequential([\n","    base_model,\n","    global_average_layer,\n","    prediction_layer\n","])\n","\n","# Compile the model\n","model.compile(\n","    optimizer='adam',\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","# Train the model\n","history = model.fit(train_batches, validation_data=validation_batches, epochs=10)\n","\n","# Evaluate the model on the testing dataset\n","testing_batches = testing_data.map(format_image).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n","test_loss, test_accuracy = model.evaluate(testing_batches)\n","print(\"Test Accuracy:\", test_accuracy)\n","print(\"Test Loss:\", test_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJCRgPdwL27H"},"outputs":[],"source":["# Define the path where you want to save the model\n","model_path = '/content/modelsaved/'\n","\n","# Save the model\n","tf.keras.models.save_model(model, model_path)\n","\n","# Print the path where the model is saved\n","print(\"Model saved at:\", model_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BYEqwRfQrp9a"},"outputs":[],"source":["import gradio as gr\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Load the pre-trained model\n","model = tf.keras.models.load_model(\"/content/modelsaved\")  # Replace with the actual path to your model file\n","IMAGE_RES = 224  # Assuming this is the image resolution used during training\n","\n","# Create a dictionary to map class indices to class names\n","class_names = [\"Aloevera\", \"Amla\", \"Amruthaballi\", \"Arali\", \"Astma_weed\", \"Badipala\", \"Balloon_Vine\", \"Bamboo\",\n","               \"Beans\", \"Betel\", \"Bhrami\", \"Bringaraja\", \"Caricature\", \"Castor\", \"Catharanthus\", \"Chakte\",\n","               \"Chilly\", \"Citron lime (herelikai)\", \"Coffee\", \"Common rue(naagdalli)\", \"Coriender\", \"Curry\",\n","               \"Doddpathre\", \"Drumstick\", \"Ekka\", \"Eucalyptus\", \"Ganigale\", \"Ganike\", \"Gasagase\", \"Ginger\",\n","               \"Globe Amarnath\", \"Guava\", \"Henna\", \"Hibiscus\", \"Honge\",\"Insulin\", \"Jackfruit\", \"Jasmine\", \"Kambajala\",\n","               \"Kasambruga\", \"Kohlrabi\", \"Lantana\", \"Lemon\", \"Lemongrass\", \"Malabar_Nut\", \"Malabar_Spinach\", \"Mango\",\n","               \"Marigold\", \"Mint\", \"Neem\", \"Nelavembu\", \"Nerale\", \"Nooni\", \"Onion\", \"Padri\", \"Palak(Spinach)\", \"Papaya\",\n","               \"Parijatha\", \"Pea\", \"Pepper\", \"Pomoegranate\", \"Pumpkin\", \"Raddish\", \"Rose\", \"Sampige\", \"Sapota\",\n","               \"Seethaashoka\", \"Seethapala\", \"Spinach1\", \"Tamarind\", \"Taro\", \"Tecoma\", \"Thumbe\", \"Tomato\", \"Tulsi\",\n","               \"Turmeric\", \"ashoka\", \"camphor\", \"kamakasturi\", \"kepala\"]  # Replace with your actual class names\n","\n","# Load the uses of the plants from a text file\n","def load_plant_uses(file_path):\n","    plant_uses = {}\n","    with open(file_path, 'r') as file:\n","        for line in file:\n","            name, uses = line.strip().split(',', 1)\n","            plant_uses[name] = uses\n","    return plant_uses\n","\n","# Load the plant uses into a dictionary\n","plant_uses = load_plant_uses('/content/plant_uses.txt')  # Replace with the actual path to your text file\n","\n","def preprocess_image(input_image):\n","    # Load the image and convert it to an array\n","    input_image_array = img_to_array(input_image)\n","\n","    # Resize the image to match the model's input shape\n","    input_image_array = tf.image.resize(input_image_array, (IMAGE_RES, IMAGE_RES))\n","\n","    # Normalize the image\n","    input_image_array = input_image_array / 255.0\n","\n","    # Add batch dimension\n","    input_image_array = np.expand_dims(input_image_array, axis=0)\n","\n","    return input_image_array\n","\n","def classify_image(input_image):\n","    try:\n","        # Preprocess the input image\n","        input_image_array = preprocess_image(input_image)\n","\n","        # Make predictions\n","        predictions = model.predict(input_image_array)\n","        predicted_class_index = np.argmax(predictions, axis=1)[0]\n","        predicted_class_name = class_names[predicted_class_index]\n","\n","        # Get the uses of the predicted plant\n","        uses = plant_uses.get(predicted_class_name, \"No uses found\")\n","\n","        return f\"{predicted_class_name}: {uses}\"\n","    except Exception as e:\n","        return str(e)  # Return the error message as output\n","\n","# Define the Gradio interface\n","gr.Interface(fn=classify_image, inputs=\"image\", outputs=\"text\", title=\"Medicinal Plant Detection\").launch()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fT97C0OdnKz_"},"outputs":[],"source":["pip install telepot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZMCobgp7_gf"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","import numpy as np\n","import telepot\n","from telepot.loop import MessageLoop\n","from io import BytesIO\n","import requests\n","\n","# Load the pre-trained model\n","model = tf.keras.models.load_model(\"/content/modelsaved\")  # Replace with the actual path to your model file\n","IMAGE_RES = 224  # Assuming this is the image resolution used during training\n","\n","# Create a dictionary to map class indices to class names\n","class_names = [\"Aloevera\", \"Amla\", \"Amruthaballi\", \"Arali\", \"Astma_weed\", \"Badipala\", \"Balloon_Vine\", \"Bamboo\",\n","               \"Beans\", \"Betel\", \"Bhrami\", \"Bringaraja\", \"Caricature\", \"Castor\", \"Catharanthus\", \"Chakte\",\n","               \"Chilly\", \"Citron lime (herelikai)\", \"Coffee\", \"Common rue(naagdalli)\", \"Coriender\", \"Curry\",\n","               \"Doddpathre\", \"Drumstick\", \"Ekka\", \"Eucalyptus\", \"Ganigale\", \"Ganike\", \"Gasagase\", \"Ginger\",\n","               \"Globe Amarnath\", \"Guava\", \"Henna\", \"Hibiscus\", \"Honge\",\"Insulin\", \"Jackfruit\", \"Jasmine\", \"Kambajala\",\n","               \"Kasambruga\", \"Kohlrabi\", \"Lantana\", \"Lemon\", \"Lemongrass\", \"Malabar_Nut\", \"Malabar_Spinach\", \"Mango\",\n","               \"Marigold\", \"Mint\", \"Neem\", \"Nelavembu\", \"Nerale\", \"Nooni\", \"Onion\", \"Padri\", \"Palak(Spinach)\", \"Papaya\",\n","               \"Parijatha\", \"Pea\", \"Pepper\", \"Pomoegranate\", \"Pumpkin\", \"Raddish\", \"Rose\", \"Sampige\", \"Sapota\",\n","               \"Seethaashoka\", \"Seethapala\", \"Spinach1\", \"Tamarind\", \"Taro\", \"Tecoma\", \"Thumbe\", \"Tomato\", \"Tulsi\",\n","               \"Turmeric\", \"ashoka\", \"camphor\", \"kamakasturi\", \"kepala\"]  # Replace with your actual class names\n","\n","# Load the uses of the plants from a text file\n","def load_plant_uses(file_path):\n","    plant_uses = {}\n","    with open(file_path, 'r') as file:\n","        for line in file:\n","            name, uses = line.strip().split(',', 1)\n","            plant_uses[name] = uses\n","    return plant_uses\n","\n","# Load the plant uses into a dictionary\n","plant_uses = load_plant_uses('/content/plant_uses.txt')  # Replace with the actual path to your text file\n","\n","def preprocess_image(input_image):\n","    # Convert the image to an array\n","    input_image_array = img_to_array(input_image)\n","\n","    # Resize the image to match the model's input shape\n","    input_image_array = tf.image.resize(input_image_array, (IMAGE_RES, IMAGE_RES))\n","\n","    # Normalize the image\n","    input_image_array = input_image_array / 255.0\n","\n","    # Add batch dimension\n","    input_image_array = np.expand_dims(input_image_array, axis=0)\n","\n","    return input_image_array\n","\n","def classify_image(input_image):\n","    try:\n","        # Preprocess the input image\n","        input_image_array = preprocess_image(input_image)\n","\n","        # Make predictions\n","        predictions = model.predict(input_image_array)\n","        predicted_class_index = np.argmax(predictions, axis=1)[0]\n","        predicted_class_name = class_names[predicted_class_index]\n","\n","        # Get the uses of the predicted plant\n","        uses = plant_uses.get(predicted_class_name, \"No uses found\")\n","\n","        return f\"{predicted_class_name}: {uses}\"\n","    except Exception as e:\n","        return str(e)  # Return the error message as output\n","\n","# Define the function to handle incoming Telegram messages\n","def handle(msg):\n","    content_type, chat_type, chat_id = telepot.glance(msg)\n","\n","    if content_type == 'photo':\n","        # Get the largest photo size\n","        photo = msg['photo'][-1]\n","        file_id = photo['file_id']\n","\n","        # Get the file path\n","        file_info = bot.getFile(file_id)\n","        file_path = file_info['file_path']\n","\n","        # Download the image\n","        image_url = f\"https://api.telegram.org/file/bot{TOKEN}/{file_path}\"\n","        response = requests.get(image_url)\n","        image = load_img(BytesIO(response.content), target_size=(IMAGE_RES, IMAGE_RES))\n","\n","        # Classify the image\n","        result = classify_image(image)\n","\n","        # Send the result back to the user\n","        bot.sendMessage(chat_id, result)\n","    else:\n","        bot.sendMessage(chat_id, \"Please send a photo of a plant for classification.\")\n","\n","# Replace 'YOUR_BOT_TOKEN' with your actual bot token\n","TOKEN = '6879192190:AAGIBMNBoql4BNa-u7qMqtA7VAMfrwUwKiI'\n","bot = telepot.Bot(TOKEN)\n","\n","# Start listening to Telegram messages\n","MessageLoop(bot, handle).run_as_thread()\n","\n","print('Listening for incoming messages...')\n","\n","# Keep the program running\n","import time\n","while True:\n","    time.sleep(10)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}